### Setting up in Google colab ###
import os
from google.colab import drive
drive.mount("/content/drive", force_remount=True)
os.chdir("/content/drive/MyDrive/Deep_Learning_HW_3")
print("Working directory:", os.getcwd())

### Imports ###
import tensorflow as tf
import numpy as np
import pandas as pd
import os
import glob
import matplotlib.pyplot as plt
import cv2
import random
import tabulate
import seaborn as sns
from tensorflow.keras import layers, models, Input, Model, utils
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Add
from tensorflow.keras.utils import plot_model
from sklearn.metrics import confusion_matrix, classification_report
from tabulate import tabulate

#Global Variables
TRIAL_NAME = "CNN_best_model_HW3"
DATA_DIR = "Project_Data_Final"
IMG_SIZE = (64, 64)
BATCH_SIZE = 64
EPOCHS = 150

# Toggle options for different techniques
USE_DATA_AUGMENTATION = 1
DROP_RATE = 0.4
USE_BATCH_NORM = 1
USE_INCEPTION = 0
USE_RESIDUAL = 0

def decode_image(img_path, label, augment=False):
    # Loads image and does preprocessing

    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)  # Convert jpgs to pngs
    img = tf.image.resize(img, IMG_SIZE)  # Resize to expected size (ex. 64 by 64)
    img = img / 255.0  # Normalize to be between 0 and 1
    return img, label

def augment_image(image, label): # Augment files - MOVE TO WITHIN FUNCTION AFTER?

    if tf.random.uniform(()) > 0.5: # 50% of the time
        image = tf.image.flip_left_right(image)  # Flip across x axis

    if tf.random.uniform(()) > 0.5:
        image = tf.image.flip_up_down(image)  # Flip across y axis

    if tf.random.uniform(()) > 0.5:
        num_rotations = tf.random.uniform(shape=[], minval=1, maxval=4, dtype=tf.int32)
        image = tf.image.rot90(image, k=num_rotations)  # Rotate 90/180/270 degrees

    return image, label

def augment_dataset(dataset, num_samples=5):
    # Calls augmentation and then visualizes them

    # Set up figure
    plt.figure(figsize=(10, num_samples * 2))

    for i, (img, label) in enumerate(dataset.take(num_samples)):
        img = img.numpy()  # Change Tensor to NumPy array
        augmented_img, _ = augment_image(tf.convert_to_tensor(img), label)
        # Apply augmentation
        # Don't care about the label, only the image

        plt.subplot(num_samples, 2, 2 * i + 1) # Plot on odd (left) side
        plt.imshow(img)
        plt.title(f"Original (Label: {int(label.numpy())})") # Label as 0 or 1 (unmixed or mixed)
        plt.axis("off")

        plt.subplot(num_samples, 2, 2 * i + 2) # Plot on even (right) side
        plt.imshow(augmented_img.numpy())
        plt.title("Augmented")
        plt.axis("off")

    plt.show()

    return dataset.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE) # Apply optimally to all data

def load_data():

    # Load and sort all images
    all_images = sorted(glob.glob('/content/drive/MyDrive/Deep_Learning_HW_3/new_data/*.png'))

    # Separate properly
    unmixed_images = []
    mixed_images = []

    for img in all_images:
        if "unmixed" in img:
            unmixed_images.append(img)
        elif "mixed" in img:
            mixed_images.append(img)

    # NO shuffle -- preserve sequential order!

    # Split function (no shuffle)
    def split_data(image_list):
        n_total = len(image_list)
        n_train = int(0.7 * n_total)
        n_val = int(0.15 * n_total)
        n_test = n_total - n_train - n_val  # remainder

        train = image_list[:n_train]
        val = image_list[n_train:n_train+n_val]
        test = image_list[n_train+n_val:]
        
        return train, val, test

    # Split unmixed
    train_unmixed, val_unmixed, test_unmixed = split_data(unmixed_images)

    # Split mixed
    train_mixed, val_mixed, test_mixed = split_data(mixed_images)

    # Print summary
    print(f"Train unmixed: {len(train_unmixed)}")
    print(f"Val unmixed: {len(val_unmixed)}")
    print(f"Test unmixed: {len(test_unmixed)}")

    print(f"Train mixed: {len(train_mixed)}")
    print(f"Val mixed: {len(val_mixed)}")
    print(f"Test mixed: {len(test_mixed)}")

    # Assign labels
    train_paths = train_unmixed + train_mixed
    train_labels = [0] * len(train_unmixed) + [1] * len(train_mixed)

    val_paths = val_unmixed + val_mixed
    val_labels = [0] * len(val_unmixed) + [1] * len(val_mixed)

    test_paths = test_unmixed + test_mixed
    test_labels = [0] * len(test_unmixed) + [1] * len(test_mixed)

    # Image preprocessing function
    def process_path(path, label):
        img_raw = tf.io.read_file(path)
        img_decoded = tf.image.decode_png(img_raw, channels=3)
        img_resized = tf.image.resize(img_decoded, IMG_SIZE)
        img_normalized = img_resized / 255.0
        return img_normalized, label

    # Training dataset
    train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))
    train_ds = train_ds.shuffle(buffer_size=len(train_paths))
    train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)

    if USE_DATA_AUGMENTATION:
        train_ds = augment_dataset(train_ds)  # Augment preprocessed images

    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    train_ds = train_ds.cache()

    # Validation dataset
    val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))
    val_ds = val_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)
    val_ds = val_ds.cache()
    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    # Test dataset
    test_ds = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))
    test_ds = test_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)
    test_ds = test_ds.cache()
    test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    print(f"Full dataset loaded and set up. It contains {len(train_paths)} training samples, {len(val_paths)} validation samples, {len(test_paths)} test samples.")
    return train_ds, val_ds, test_ds

def residual_block(x, filters):
    """Residual block with properly structured skip connections and BatchNorm before ReLU."""
    shortcut = x  # Save input for the residual (skip) connection

    # Match dimensions with 1x1 convolution if input depth doesn't equal number of filters
    if x.shape[-1] != filters:
        shortcut = layers.Conv2D(filters, (1, 1), padding='same')(shortcut)
        if USE_BATCH_NORM:
            shortcut = layers.BatchNormalization()(shortcut)

    # First convolutional layer
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    if USE_BATCH_NORM:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # Second convolutional layer
    x = layers.Conv2D(filters, (3, 3), padding='same')(x)
    if USE_BATCH_NORM:
        x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # Add skip connection
    x = layers.Add()([x, shortcut])
    x = layers.Activation('relu')(x)

    return x

def inception_module(x, filters, dropout_rate=0.3):
    """Inception module with batch normalization and dropout."""

    # Branch 1: 1x1 convolution
    conv1x1 = layers.Conv2D(filters, (1, 1), padding='same')(x)
    conv1x1 = layers.BatchNormalization()(conv1x1)
    conv1x1 = layers.Activation('relu')(conv1x1)

    # Branch 2: 3x3 convolution
    conv3x3 = layers.Conv2D(filters, (3, 3), padding='same')(x)
    conv3x3 = layers.BatchNormalization()(conv3x3)
    conv3x3 = layers.Activation('relu')(conv3x3)

    # Branch 3: 5x5 convolution
    conv5x5 = layers.Conv2D(filters, (5, 5), padding='same')(x)
    conv5x5 = layers.BatchNormalization()(conv5x5)
    conv5x5 = layers.Activation('relu')(conv5x5)

    # Branch 4: Max pooling followed by 1x1 convolution
    pool_proj = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)
    pool_proj = layers.Conv2D(filters, (1, 1), padding='same')(pool_proj)
    pool_proj = layers.BatchNormalization()(pool_proj)
    pool_proj = layers.Activation('relu')(pool_proj)

    # Concatenate all branches
    output = layers.Concatenate()([conv1x1, conv3x3, conv5x5, pool_proj])

    # Dropout to reduce overfitting
    output = layers.Dropout(dropout_rate)(output)

    return output

def build_model(input_shape=(64, 64, 3)):
    """Unified CNN model with configurable techniques."""
    inputs = Input(shape=input_shape)

    # Data augmentation
    # x = layers.Rescaling(1./255)(inputs)
    x = inputs

    # Initial Conv Layer
    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)
    x = layers.MaxPooling2D((2, 2))(x)

    # Inception + Residual Blocks
    if USE_INCEPTION:
        x = inception_module(x, 32)

    if USE_RESIDUAL:
        x = residual_block(x, 64)

    x = layers.MaxPooling2D((2, 2))(x)

    if USE_INCEPTION:
        x = inception_module(x, 64)

    if USE_RESIDUAL:
        x = residual_block(x, 128)

    x = layers.MaxPooling2D((2, 2))(x)

    # Fully Connected Layers
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation='relu')(x)
    x = layers.Dropout(DROP_RATE)(x)
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dropout(DROP_RATE)(x)

    outputs = layers.Dense(1, activation='sigmoid')(x)  # Binary classification

    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def evaluate_model(model, test_ds):
    """Evaluates the model and returns predictions, true labels, test accuracy, and test loss."""
    all_labels = []
    all_predictions = []

    for images, labels in test_ds:
        preds = model.predict(images)  # Get predictions
        preds = (preds > 0.5).astype("int32")  # Convert to binary (0 or 1)

        all_labels.extend(labels.numpy())  # Store true labels
        all_predictions.extend(preds.flatten())  # Store predicted labels

    all_labels = np.array(all_labels)
    all_predictions = np.array(all_predictions)

    # Compute correctly classified instances
    mixed_correct = np.sum((all_predictions == 1) & (all_labels == 1))
    unmixed_correct = np.sum((all_predictions == 0) & (all_labels == 0))
    mixed_total = np.sum(all_labels == 1)
    unmixed_total = np.sum(all_labels == 0)

    # Compute test accuracy
    total_samples = mixed_total + unmixed_total
    test_accuracy = (mixed_correct + unmixed_correct) / total_samples if total_samples > 0 else 0.0

    # Compute test loss
    test_loss = model.evaluate(test_ds, verbose=0)[0]

    # Print classification results
    print(f"Correctly classified 'mixed' images: {mixed_correct} / {mixed_total}")
    print(f"Correctly classified 'unmixed' images: {unmixed_correct} / {unmixed_total}")

    return all_predictions, all_labels, test_accuracy, test_loss

def plot_confusion_matrix(y_true, y_pred, class_names, save_path):
    """Plots a confusion matrix, prints it, and saves it as an image."""

    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title("Confusion Matrix")

    # Save image to file
    plt.savefig(save_path)
    print(f"Confusion matrix saved as '{save_path}'.")

    plt.show()

def print_classification_report_table(y_true, y_pred):
    """Prints a well-formatted classification report."""

    # Generate classification report and convert to DataFrame
    report_dict = classification_report(y_true, y_pred, target_names=["Unmixed", "Mixed"], output_dict=True)
    # output_dict=True gives you a structured dictionary with Precision, recall, F1-score, and support for each class.
    # Also gives macro/micro/weighted averages
    # All stored as key-value pairs (perfect for converting to a table)
    report_df = pd.DataFrame(report_dict).transpose()
    # pd.DataFrame(...).transpose() turns it into a clean table by flipping rows and columns
    # The tabulate function just helps with the aestetics

    # Round values for better readability
    report_df = report_df.round(4)

    # Print as a formatted table
    print("\nClassification Report:")
    print(tabulate(report_df, headers="keys", tablefmt="grid", floatfmt=".4f"))  # Cleaner grid format

    return report_df  # Return it for saving

print("Loading data:")
train_ds, val_ds, test_ds = load_data()

print("Creating model:")
model = build_model()
model.summary()

# Generate and save the model architecture diagram
arch_path = "model_architecture_" + TRIAL_NAME + ".png"
#arch_path = "model_architecture.png"
plot_model(model, to_file=arch_path, show_shapes=True, show_layer_names=True)
arch_print = "Model architecture saved as " + arch_path + "."
print(arch_print)

print("Training model:")
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)
early_stopping = EarlyStopping(monitor="val_loss", patience=10,restore_best_weights=True, verbose=1)

# Warm-up pass
for x, y in train_ds.take(1):
    _ = model(x)

history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[lr_scheduler, early_stopping])

print("Testing model:")
all_predictions, all_labels, test_accuracy, test_loss = evaluate_model(model, test_ds)

# Print results
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

# Generate and save confusion matrix
print("Confusion matrix:")
conf_path = "confusion_matrix_" + TRIAL_NAME + ".png"
#conf_path = "confusion_matrix.png"
plot_confusion_matrix(all_labels, all_predictions, class_names=["Unmixed", "Mixed"], save_path=conf_path)

# Make classification report
print("Classification report:")
report_df = print_classification_report_table(all_labels, all_predictions)

# Save epochs for next step
epochs = range(len(history.history['loss']))

# Plot Loss with Test Loss Reference
plt.figure(figsize=(10,5))
plt.plot(epochs, history.history['loss'], label='Training Loss', color='blue')
plt.plot(epochs, history.history['val_loss'], label='Validation Loss', color='orange')
plt.axhline(y=test_loss, color='red', linestyle=':', label=f"Test Loss: {test_loss:.4f}")  # Horizontal test loss line
plt.title('Model Loss Across Epochs')
plt.xlabel('Epoch Number')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Plot Accuracy with Test Accuracy Reference
plt.figure(figsize=(10,5))
plt.plot(epochs, history.history['accuracy'], label='Training Accuracy', color='blue')
plt.plot(epochs, history.history['val_accuracy'], label='Validation Accuracy', color='orange')
plt.axhline(y=test_accuracy.mean(), color='red', linestyle=':', label=f"Test Accuracy: {test_accuracy.mean():.2f}%")
plt.title('Model Accuracy Across Epochs')
plt.xlabel('Epoch Number')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

model_name = "my_model_" + TRIAL_NAME + ".keras"
model.save(model_name)
print_model_name = "Model saved as " + model_name + "."
print(print_model_name)
